# Prac3 â€“ OOP Â· RegresiÃ³n Lineal (Java)

Este README documenta la prÃ¡ctica implementada en **Java** con dos programas:

- **`Helados.java`**: **regresiÃ³n lineal simple** (p.ej., ventas de helados vs temperatura) usando `Ice_cream_selling_data.csv`.
- **`ExamScores.java`**: **regresiÃ³n lineal mÃºltiple** (p.ej., notas vs mÃºltiples factores) usando `student_exam_scores.csv`.

Incluye una **explicaciÃ³n de los cÃ³digos**, **resultados** obtenidos (estimados con el mismo enfoque usando los CSV provistos) y una secciÃ³n de **problemas y soluciones** que suelen aparecer al desarrollarlos.

> Nota: este README asume que cada CSV es **numÃ©rico** y que la **Ãºltima columna es la variable objetivo (y)** y las anteriores son **features (X)**.

---

## ğŸ“ Estructura del repositorio

```
.
â”œâ”€â”€ Helados.java
â”œâ”€â”€ ExamScores.java
â”œâ”€â”€ Ice_cream_selling_data.csv
â”œâ”€â”€ student_exam_scores.csv
â””â”€â”€ README.md   (este archivo)
```

---

## ğŸ§  DiseÃ±o OO de la soluciÃ³n

Ambos programas siguen la misma idea OO (aunque en archivos separados):

- **Atributos (estado):**
  - `weights[]`: coeficientes de la recta/hiperplano.
  - `bias`: tÃ©rmino independiente.
- **Comportamiento (mÃ©todos):**
  - `fit(X, y, ...)`: estima `weights` y `bias`. Puede hacerse con **gradiente descendente** o con **ecuaciÃ³n normal** (mÃ­nimos cuadrados).
  - `predict(X)`: devuelve `y_hat = XÂ·w + b`.
  - `score(X, y)`: calcula un mÃ©trico de error (p.ej., **MSE**).
  - `data_scaling(...)` (si aplica): estandariza X e y (**z-score**) y guarda `Î¼` y `Ïƒ` para volver a la escala original.

**`Helados.java` (simple regression):**
- Carga un par `(x, y)` por fila del CSV.
- Ajusta una recta `y â‰ˆ w*x + b`.
- Reporta `w`, `b`, MSE y (opcional) RÂ².

**`ExamScores.java` (multiple regression):**
- Carga varias features por fila y la salida `y` en la Ãºltima columna.
- Ajusta `y â‰ˆ XÂ·w + b` con `w` vector de tamaÃ±o `d`.
- Reporta `w`, `b`, MSE y (opcional) RÂ².

---

## â–¶ï¸ CÃ³mo compilar y ejecutar (Java)

```bash
# compilar
javac Helados.java
javac ExamScores.java

# ejecutar
java Helados
java ExamScores
```

AsegÃºrate de que los CSV se ubiquen junto a los `.java` o que el cÃ³digo use rutas correctas.

---

## âœ… Resultados obtenidos (con los CSV provistos)

> Para cuantificar resultados aquÃ­, estimÃ© los modelos por **mÃ­nimos cuadrados** usando los mismos CSV.
> Si las implementaciones Java usan **gradiente descendente** y/o **escalado**, los nÃºmeros pueden variar levemente pero deberÃ­an ser **consistentes**.

### 1) `Helados.java` â€” RegresiÃ³n lineal **simple**
- **n muestras**: 49
- **d features**: 1
- **weights**: `[-0.718679]`
- **bias**: `16.9445`
- **MSE (train)**: `141.918`
- **MSE (test)**:  `150.995`
- **RÂ² (train)**:  `0.0270728`
- **RÂ² (test)**:   `-0.0799539`


### 2) `ExamScores.java` â€” RegresiÃ³n lineal **mÃºltiple**
- **n muestras**: 200
- **d features**: 4
- **weights**: `[1.55882, 0.985285, 0.113807, 0.186539]`
- **bias**: `-3.6282`
- **MSE (train)**: `7.08021`
- **MSE (test)**:  `8.43169`
- **RÂ² (train)**:  `0.853654`
- **RÂ² (test)**:   `0.759912`


> InterpretaciÃ³n rÃ¡pida:  
> - **MSE**: error cuadrÃ¡tico medio (â†“ mejor).  
> - **RÂ²**: proporciÃ³n de varianza explicada (1 = perfecto, 0 = igual que el promedio).  
> - Si **RÂ² (test)** es cercano a **RÂ² (train)** y razonablemente alto, el modelo generaliza bien.

---

## ğŸ” ExplicaciÃ³n tÃ©cnica (paso a paso)

1. **Carga de datos**: leer CSV en memoria, separando X (todas las columnas menos la Ãºltima) y y (Ãºltima).
2. **(Opcional) Escalado**:
   - `X_scaled = (X - Î¼_X) / Ïƒ_X`, `y_scaled = (y - Î¼_y) / Ïƒ_y`.
   - Mejora la **convergencia** si usas gradiente; en mÃ­nimos cuadrados no es imprescindible.
3. **Ajuste**:
   - **EcuaciÃ³n normal**: resolver `Î¸ = (X_augáµ€ X_aug)â»Â¹ X_augáµ€ y` con `X_aug = [1 | X]`; `Î¸ = [b; w]`.
   - **Gradiente descendente**: iterar `Î¸ := Î¸ - Î· âˆ‡MSE` hasta converger.
4. **PredicciÃ³n**: `Å· = XÂ·w + b`.
5. **EvaluaciÃ³n**: calcular **MSE** y (si se desea) **RÂ²** en train/test.

---

## ğŸ› ï¸ Problemas reales y soluciones aplicadas

1. **CSV con encabezados / strings**  
   - *SÃ­ntoma*: `NumberFormatException` al parsear.  
   - *SoluciÃ³n*: ignorar la primera fila si no es numÃ©rica; validar cada token con `try/catch` y reportar filas invÃ¡lidas.

2. **Separador decimal y locales**  
   - *SÃ­ntoma*: decimales con **coma** `1,23` no se parsean con `Double.parseDouble`.  
   - *SoluciÃ³n*: reemplazar `,` â†’ `.` o usar `NumberFormat` con `Locale.US`.

3. **Rutas con espacios**  
   - *SÃ­ntoma*: el archivo no se encuentra.  
   - *SoluciÃ³n*: evitar espacios en nombres de archivo o envolver ruta entre comillas/usar `Paths.get(...)`.

4. **Divergencia del gradiente (si se usÃ³)**  
   - *SÃ­ntoma*: MSE sube o `NaN`.  
   - *SoluciÃ³n*: reducir `learningRate`, aplicar **z-score** y aumentar epochs gradualmente.

5. **Desajuste de dimensiones**  
   - *SÃ­ntoma*: `IndexOutOfBounds` o longitudes distintas entre X e y.  
   - *SoluciÃ³n*: validar que **todas las filas** tengan el mismo nÃºmero de columnas; asegurar que `X[i].length == d` y `y.length == n`.

6. **Sobreajuste**  
   - *SÃ­ntoma*: RÂ² (train) alto pero RÂ² (test) bajo.  
   - *SoluciÃ³n*: separar train/test, simplificar features o aÃ±adir regularizaciÃ³n L2 (Ridge) si se permite.

---

## ğŸ“Œ Conclusiones

1. La **formulaciÃ³n OO** (clase con `weights`/`bias` y mÃ©todos `fit/predict/score`) permite cambiar el **mÃ©todo de entrenamiento** sin tocar el resto del cÃ³digo.  
2. El **escalado** mejora la estabilidad cuando se usa **gradiente descendente** y facilita hiperparÃ¡metros razonables.  
3. Separar **train/test** y reportar **MSE/RÂ²** evita impresiones engaÃ±osas y mide la **generalizaciÃ³n** del modelo.

---

## ğŸ“ Datos y reproducibilidad

- Este README fue generado inspeccionando los CSV provistos y calculando coeficientes por **mÃ­nimos cuadrados** (normal equation).  
- Si deseas, actualiza los nÃºmeros con la **salida real** de tus programas Java (copiando el log de consola aquÃ­).
